{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c400f4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: File saved successfully to C:\\Users\\razhd\\Downloads\\Mike Pence - columns cleaned.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# file paths\n",
    "input_file = r\"C:\\Users\\razhd\\Downloads\\Mike Pence - to clean.xlsx\"\n",
    "output_file = r\"C:\\Users\\razhd\\Downloads\\Mike Pence - columns cleaned.xlsx\"\n",
    "\n",
    "# sheets to process\n",
    "sheets_to_process = [\"before visit\", \"visit day\", \"after visit\"]\n",
    "\n",
    "# columns and their new names\n",
    "desired_columns = [\n",
    "    \"url\",\n",
    "    \"published\",\n",
    "    \"title\",\n",
    "    \"content\",\n",
    "    \"source_type\",\n",
    "    \"post_type\",\n",
    "    \"engagement\",\n",
    "    \"matched_profile\",\n",
    "    \"extra_author_attributes.name\",\n",
    "    \"source_extended_attributes.twitter_followers\",\n",
    "    \"source_extended_attributes.instagram_followers\",\n",
    "    \"×œ×™× ×§\",\n",
    "    \"×¡× ×˜×™×× ×˜\",\n",
    "    \"×”×ª×™×™×—×¡×•×ª ×œ×‘×™×§×•×¨\",\n",
    "    \"×”×¢×¨×•×ª (×”×× ×—×¨×™×’? ×”×× ×™×© ×ª××•× ×”/×¡×¨×˜×•×Ÿ ××”×¡×™×•×¨?)\"\n",
    "]\n",
    "\n",
    "new_column_names = {\n",
    "    \"url\": \"URL\",\n",
    "    \"published\": \"Published Date\",\n",
    "    \"title\": \"Title\",\n",
    "    \"content\": \"Content\",\n",
    "    \"source_type\": \"Source_Type\",\n",
    "    \"post_type\": \"Content_Type\",\n",
    "    \"engagement\": \"Engagement\",\n",
    "    \"matched_profile\": \"Keyword in Post\",\n",
    "    \"extra_author_attributes.name\": \"Author Name\",\n",
    "    \"source_extended_attributes.twitter_followers\": \"Twitter Followers\",\n",
    "    \"source_extended_attributes.instagram_followers\": \"Instagram Followers\",\n",
    "    \"×œ×™× ×§\": \"×œ×™× ×§\",\n",
    "    \"×¡× ×˜×™×× ×˜\": \"×¡× ×˜×™×× ×˜\",\n",
    "    \"×”×ª×™×™×—×¡×•×ª ×œ×‘×™×§×•×¨\": \"×”×ª×™×™×—×¡×•×ª ×œ×‘×™×§×•×¨\",\n",
    "    \"×”×¢×¨×•×ª (×”×× ×—×¨×™×’? ×”×× ×™×© ×ª××•× ×”/×¡×¨×˜×•×Ÿ ××”×¡×™×•×¨?)\":\"×”×¢×¨×•×ª (×”×× ×—×¨×™×’? ×”×× ×™×© ×ª××•× ×”/×¡×¨×˜×•×Ÿ ××”×¡×™×•×¨?)\"\n",
    "}\n",
    "\n",
    "# dictionary to store processed sheets\n",
    "processed_sheets = {}\n",
    "\n",
    "# Loop through each sheet\n",
    "for sheet in sheets_to_process:\n",
    "    df = pd.read_excel(input_file, sheet_name=sheet)\n",
    "    df = df[desired_columns]\n",
    "    df = df.rename(columns=new_column_names)\n",
    "    processed_sheets[sheet] = df\n",
    "\n",
    "# save processed sheets\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for sheet_name, df in processed_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Step 1: File saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c4af6c8-e870-4176-b37b-845ad0dd70c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully to C:\\Users\\razhd\\Downloads\\Mike Pence - Cleaned Source Type.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# file paths\n",
    "input_file = r\"C:\\Users\\razhd\\Downloads\\Mike Pence - columns cleaned.xlsx\"\n",
    "output_file = r\"C:\\Users\\razhd\\Downloads\\Mike Pence - Cleaned Source Type.xlsx\"\n",
    "\n",
    "# load sheets\n",
    "sheets = pd.read_excel(input_file, sheet_name=None)  # Load all sheets as a dictionary\n",
    "\n",
    "# dictionary to store processed sheets\n",
    "processed_sheets = {}\n",
    "\n",
    "# loop through each sheet\n",
    "for sheet_name, df in sheets.items():\n",
    "    # Clean the Source Type column if it exists\n",
    "    if \"Source_Type\" in df.columns:\n",
    "        df[\"Source_Type\"] = df[\"Source_Type\"].replace({\n",
    "            \"SOCIALMEDIA,SOCIALMEDIA_INSTAGRAM\": \"Instagram\",\n",
    "            \"SOCIALMEDIA,SOCIALMEDIA_TWITTER\": \"Twitter\"\n",
    "        })\n",
    "    processed_sheets[sheet_name] = df\n",
    "\n",
    "# Save the processed sheets to a new Excel file\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for sheet_name, df in processed_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"File saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a63a77-b008-4f2b-843d-8837e043a674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: File saved successfully to C:\\Users\\razhd\\Downloads\\Mike Pence - Keywords Added.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = r\"C:\\Users\\razhd\\Downloads\\Mike Pence - Cleaned Source Type.xlsx\"\n",
    "output_file = r\"C:\\Users\\razhd\\Downloads\\Mike Pence - Keywords Added.xlsx\"\n",
    "\n",
    "# Specify the sheets to process\n",
    "sheets_to_process = [\"before visit\",\"visit day\", \"after visit\"]\n",
    "\n",
    "# Function to extract keywords from the \"matched_profile\" column\n",
    "def extract_keywords(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    keywords = []\n",
    "    # Split by comma to get individual parts\n",
    "    parts = str(text).split(',')\n",
    "    \n",
    "    # Look for the pattern \"KPMG / keyword\"\n",
    "    for part in parts:\n",
    "        if '××™×œ×•×ª ××¤×ª×— - ×¡×™×•×¨×™× - KPMG /' in part:\n",
    "            # Extract the keyword after the last forward slash\n",
    "            keyword = part.split('/')[-1].strip()\n",
    "            if keyword:\n",
    "                keywords.append(keyword)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_keywords = []\n",
    "    for keyword in keywords:\n",
    "        if keyword not in seen:\n",
    "            seen.add(keyword)\n",
    "            unique_keywords.append(keyword)\n",
    "    \n",
    "    return \", \".join(unique_keywords)\n",
    "\n",
    "# Dictionary to store processed sheets\n",
    "processed_sheets = {}\n",
    "\n",
    "# Loop through each sheet\n",
    "for sheet in sheets_to_process:\n",
    "    # Read the sheet\n",
    "    df = pd.read_excel(input_file, sheet_name=sheet)\n",
    "    \n",
    "    # Clean and extract keywords from the matched_profile column\n",
    "    if \"matched_profile\" in df.columns:\n",
    "        df[\"Keyword in Post\"] = df[\"matched_profile\"].apply(extract_keywords)\n",
    "    elif \"Keyword in Post\" in df.columns:\n",
    "        df[\"Keyword in Post\"] = df[\"Keyword in Post\"].apply(extract_keywords)\n",
    "    \n",
    "    processed_sheets[sheet] = df\n",
    "\n",
    "# Save the processed sheets\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for sheet_name, df in processed_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Step 3: File saved successfully to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba4e36d-6f7a-449e-9202-8d074177ab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully to C:\\Users\\razhd\\Downloads\\Mike Pence - For Tamar.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = r\"C:\\Users\\razhd\\Downloads\\Mike Pence - Keywords Added.xlsx\"\n",
    "output_file = r\"C:\\Users\\razhd\\Downloads\\Mike Pence - For Tamar.xlsx\"\n",
    "\n",
    "# Load all sheets into a dictionary\n",
    "sheets = pd.read_excel(input_file, sheet_name=None)\n",
    "\n",
    "# Dictionary to store processed sheets\n",
    "processed_sheets = {}\n",
    "\n",
    "# Loop through each sheet\n",
    "for sheet_name, df in sheets.items():\n",
    "    # Check if \"Content\" and \"Source_Type\" columns exist\n",
    "    if \"Content\" in df.columns and \"Source_Type\" in df.columns:\n",
    "        # Calculate index to insert the new column after \"Content\"\n",
    "        content_index = df.columns.get_loc(\"Content\") + 1\n",
    "\n",
    "        # Define logic for \"Post Type\" based on the \"Content\" and \"Source_Type\" columns\n",
    "        def determine_post_type(row):\n",
    "            if row[\"Source_Type\"].strip().lower() == \"twitter\":\n",
    "                content = str(row[\"Content\"])\n",
    "                # Match 'RT' or 'QT' as separate words (case-sensitive)\n",
    "                if re.search(r'\\bRT\\b', content):\n",
    "                    return \"Retweet\"\n",
    "                elif re.search(r'\\bQT\\b', content):\n",
    "                    return \"Quote Tweet\"\n",
    "                else:\n",
    "                    return \"Post\"\n",
    "            return \"Post\"  # Default for non-Twitter rows\n",
    "\n",
    "        # Apply the logic and insert the \"Post Type\" column\n",
    "        df.insert(content_index, \"Post Type\", df.apply(determine_post_type, axis=1))\n",
    "    else:\n",
    "        print(f\"Warning: 'Content' or 'Source_Type' column not found in sheet '{sheet_name}'\")\n",
    "\n",
    "    # Add the processed sheet back to the dictionary\n",
    "    processed_sheets[sheet_name] = df\n",
    "\n",
    "# Save the processed sheets back to a new Excel file\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for sheet_name, df in processed_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"File saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "903136ab-f7b3-4abf-81d5-f700f132a118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Putin is a war criminal and only understands s...\n",
      "1     Traveled to Israelâ€™s Northern Command today to...\n",
      "2     Back home from Israel after four days I will n...\n",
      "3     Back home from Israel after four days I will n...\n",
      "4     Honored to sit down with my good friend Prime ...\n",
      "5     Traveled to Israelâ€™s Northern Command today to...\n",
      "6     This is a moment where America needs to speak ...\n",
      "7     Honored to sit down with my good friend Prime ...\n",
      "8     Itâ€™s About Time!ğŸ‡ºğŸ‡¸ US, UK launch retaliatory s...\n",
      "9     Honored to attend a Havdalah Ceremony marking ...\n",
      "10    Pence In Israel: October 7 Was â€˜Not Simply A T...\n",
      "11    Privileged to meet with President Isaac Herzog...\n",
      "12    In the face of these widening dangers, I urge ...\n",
      "13    Former US VP Mike Pence to Israel: 'The Americ...\n",
      "14    Honored to attend a Havdalah Ceremony marking ...\n",
      "15    Pence in Israel: Jewish State 'Our Most Cheris...\n",
      "16    Israel is waging a war for its very survival a...\n",
      "17    Privileged to meet with President @Isaac_Herzo...\n",
      "18    Privileged to meet with Benny Gantz to discuss...\n",
      "19    The United States of America must stand in thi...\n",
      "20    Inspiring visit to the Magen David Adom Comman...\n",
      "21    Privileged to meet with @gantzbe today in Isra...\n",
      "22    Pence to Israel Hayom: I will be Israelis' voi...\n",
      "23    Inspiring visit to the @Mdais Command Center t...\n",
      "24    Humbling to meet with Minister Katz and famili...\n",
      "25    Pence in Israel: Jewish State 'Our Most Cheris...\n",
      "26    Republicans in Congress should work to remove ...\n",
      "27    In Jerusalem, former US VP Mike Pence warns US...\n",
      "28    Humbling to meet with @Israel_katz and familie...\n",
      "29    Israel Needs Americaâ€™s Full Support, Not the B...\n",
      "30    IDF Sgt. Malespin tours site of Oct. 7 devasta...\n",
      "31    Thank You @Franklin_Graham & @SamaritansPurse!...\n",
      "32    .@SamaritansPurse is doing extraordinary work ...\n",
      "33    Check Out Our Recent Interview from Israel Ton...\n",
      "34    RT @cbnisraelaid: ASHKELON, Israel â€“ Former U....\n",
      "35    RT @all_israel_news: Former U.S. Vice Presiden...\n",
      "36    RT @SkyNews: \"America will stand with Israel t...\n",
      "37    RT @HudsonInstitute: War has broken out as a r...\n",
      "38    RT @NRO: There has never been a more important...\n",
      "39    RT @JoelCRosenberg: Lynn and I were so honored...\n",
      "40    RT @Israel_katz: ××™×™×§ ×¤× ×¡ @Mike_Pence ×”×•× ×™×“×™×“...\n",
      "41    RT @dannydanon: Today I met with former US Vic...\n",
      "42    RT @AmericanFreedom: NEW Op-ed from Paul Telle...\n",
      "43    RT @Mike_Pence: .@SamaritansPurse is doing ext...\n",
      "44    RT @gantzbe: I met today with the former Vice ...\n",
      "45    RT @Mike_Pence: Pence In Israel: October 7 Was...\n",
      "46    RT @IsraelPresident: President @Isaac_Herzog m...\n",
      "47    RT @jonah_wendt: NEW: @AmericanFreedom leads m...\n",
      "48    RT @yoavgallant: Thank you @Mike_Pence for you...\n",
      "49    RT @Jerusalem_Post: Former US VP Mike Pence vi...\n",
      "Name: Content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Content\"].head(50))  # Print first 10 rows of Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3aea820-45e4-4ec4-9a51-14f8c346d875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Instagram' 'Twitter']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Source_Type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3198aa-f9a6-4086-9084-62ac398cbe51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
