{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c400f4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: File saved successfully to C:\\Users\\razhd\\Downloads\\Stockwell Day - columns cleaned.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_file = r\"C:\\Users\\razhd\\Downloads\\Stockwell Day - to clean.xlsx\"\n",
    "output_file = r\"C:\\Users\\razhd\\Downloads\\Stockwell Day - columns cleaned.xlsx\"\n",
    "\n",
    "# Specify the sheets to process\n",
    "sheets_to_process = [\"before visit\", \"visit day\", \"after visit\"] ,
    "\n",
    "# Desired columns and their new names\n",
    "desired_columns = [\n",
    "    \"url\",\n",
    "    \"published\",\n",
    "    \"title\",\n",
    "    \"content\",\n",
    "    \"source_type\",\n",
    "    \"post_type\",\n",
    "    \"engagement\",\n",
    "    \"matched_profile\",\n",
    "    \"extra_author_attributes.name\",\n",
    "    \"source_extended_attributes.twitter_followers\",\n",
    "    \"source_extended_attributes.instagram_followers\",\n",
    "    \"לינק\",\n",
    "    \"סנטימנט\",\n",
    "    \"התייחסות לביקור\",\n",
    "    \"הערות (האם חריג? האם יש תמונה/סרטון מהסיור?)\"\n",
    "]\n",
    "\n",
    "new_column_names = {\n",
    "    \"url\": \"URL\",\n",
    "    \"published\": \"Published Date\",\n",
    "    \"title\": \"Title\",\n",
    "    \"content\": \"Content\",\n",
    "    \"source_type\": \"Source_Type\",\n",
    "    \"post_type\": \"Content_Type\",\n",
    "    \"engagement\": \"Engagement\",\n",
    "    \"matched_profile\": \"Keyword in Post\",  # Keep the same name\n",
    "    \"extra_author_attributes.name\": \"Author Name\",\n",
    "    \"source_extended_attributes.twitter_followers\": \"Twitter Followers\",\n",
    "    \"source_extended_attributes.instagram_followers\": \"Instagram Followers\",\n",
    "    \"לינק\": \"לינק\",\n",
    "    \"סנטימנט\": \"סנטימנט\",\n",
    "    \"התייחסות לביקור\": \"התייחסות לביקור\",\n",
    "    \"הערות (האם חריג? האם יש תמונה/סרטון מהסיור?)\":\"הערות (האם חריג? האם יש תמונה/סרטון מהסיור?)\"\n",
    "}\n",
    "\n",
    "# Dictionary to store processed sheets\n",
    "processed_sheets = {}\n",
    "\n",
    "# Loop through each sheet\n",
    "for sheet in sheets_to_process:\n",
    "    df = pd.read_excel(input_file, sheet_name=sheet)\n",
    "    df = df[desired_columns]\n",
    "    df = df.rename(columns=new_column_names)\n",
    "    processed_sheets[sheet] = df\n",
    "\n",
    "# Save the processed sheets\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for sheet_name, df in processed_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Step 1: File saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c4af6c8-e870-4176-b37b-845ad0dd70c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully to C:\\Users\\razhd\\Downloads\\Stockwell Day - Cleaned Source Type.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_file = r\"C:\\Users\\razhd\\Downloads\\Stockwell Day - columns cleaned.xlsx\"\n",
    "output_file = r\"C:\\Users\\razhd\\Downloads\\Stockwell Day - Cleaned Source Type.xlsx\"\n",
    "\n",
    "# Load all sheets\n",
    "sheets = pd.read_excel(input_file, sheet_name=None),
    "\n",
    "# Dictionary to store processed sheets\n",
    "processed_sheets = {}\n",
    "\n",
    "# Loop through each sheet\n",
    "for sheet_name, df in sheets.items():\n",
    "    # Clean the Source Type column if it exists\n",
    "    if \"Source_Type\" in df.columns:\n",
    "        df[\"Source_Type\"] = df[\"Source_Type\"].replace({\n",
    "            \"SOCIALMEDIA,SOCIALMEDIA_INSTAGRAM\": \"Instagram\",\n",
    "            \"SOCIALMEDIA,SOCIALMEDIA_TWITTER\": \"Twitter\"\n",
    "        })\n",
    "    processed_sheets[sheet_name] = df\n",
    "\n",
    "# Save the processed sheets to a new Excel file\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for sheet_name, df in processed_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"File saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a63a77-b008-4f2b-843d-8837e043a674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: File saved successfully to C:\\Users\\razhd\\Downloads\\Stockwell Day - Keywords Added.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = r\"C:\\Users\\razhd\\Downloads\\Stockwell Day - Cleaned Source Type.xlsx\"\n",
    "output_file = r\"C:\\Users\\razhd\\Downloads\\Stockwell Day - Keywords Added.xlsx\"\n",
    "\n",
    "# Specify the sheets to process\n",
    "sheets_to_process = [\"before visit\", \"visit day\", \"after visit\"]\n",
    "\n",
    "# Function to extract keywords from the \"matched_profile\" column\n",
    "def extract_keywords(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    keywords = []\n",
    "    # Split by comma to get individual parts\n",
    "    parts = str(text).split(',')\n",
    "    \n",
    "    # Look for the pattern \"KPMG / keyword\"\n",
    "    for part in parts:\n",
    "        if 'מילות מפתח - סיורים - KPMG /' in part:\n",
    "            # Extract the keyword after the last forward slash\n",
    "            keyword = part.split('/')[-1].strip()\n",
    "            if keyword:\n",
    "                keywords.append(keyword)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_keywords = []\n",
    "    for keyword in keywords:\n",
    "        if keyword not in seen:\n",
    "            seen.add(keyword)\n",
    "            unique_keywords.append(keyword)\n",
    "    \n",
    "    return \", \".join(unique_keywords)\n",
    "\n",
    "# Dictionary to store processed sheets\n",
    "processed_sheets = {}\n",
    "\n",
    "# Loop through each sheet\n",
    "for sheet in sheets_to_process:\n",
    "    # Read the sheet\n",
    "    df = pd.read_excel(input_file, sheet_name=sheet)\n",
    "    \n",
    "    # Clean and extract keywords from the matched_profile column\n",
    "    if \"matched_profile\" in df.columns:\n",
    "        df[\"Keyword in Post\"] = df[\"matched_profile\"].apply(extract_keywords)\n",
    "    elif \"Keyword in Post\" in df.columns:\n",
    "        df[\"Keyword in Post\"] = df[\"Keyword in Post\"].apply(extract_keywords)\n",
    "    \n",
    "    processed_sheets[sheet] = df\n",
    "\n",
    "# Save the processed sheets\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for sheet_name, df in processed_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Step 3: File saved successfully to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba4e36d-6f7a-449e-9202-8d074177ab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully to C:\\Users\\razhd\\Downloads\\Stockwell Day - Final.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = r\"C:\\Users\\razhd\\Downloads\\Stockwell Day - Keywords Added.xlsx\"\n",
    "output_file = r\"C:\\Users\\razhd\\Downloads\\Stockwell Day - For Tamar.xlsx\"\n",
    "\n",
    "# Load all sheets into a dictionary\n",
    "sheets = pd.read_excel(input_file, sheet_name=None)\n",
    "\n",
    "# Dictionary to store processed sheets\n",
    "processed_sheets = {}\n",
    "\n",
    "# Loop through each sheet\n",
    "for sheet_name, df in sheets.items():\n",
    "    # Check if \"Content\" and \"Source_Type\" columns exist\n",
    "    if \"Content\" in df.columns and \"Source_Type\" in df.columns:\n",
    "        # Calculate index to insert the new column after \"Content\"\n",
    "        content_index = df.columns.get_loc(\"Content\") + 1\n",
    "\n",
    "        # Define logic for \"Post Type\" based on the \"Content\" and \"Source_Type\" columns\n",
    "        def determine_post_type(row):\n",
    "            if row[\"Source_Type\"].strip().lower() == \"twitter\":\n",
    "                content = str(row[\"Content\"])\n",
    "                # Match 'RT' or 'QT' as separate words (case-sensitive)\n",
    "                if re.search(r'\\bRT\\b', content):\n",
    "                    return \"Retweet\"\n",
    "                elif re.search(r'\\bQT\\b', content):\n",
    "                    return \"Quote Tweet\"\n",
    "                else:\n",
    "                    return \"Post\"\n",
    "            return \"Post\"  # Default for non-Twitter rows\n",
    "\n",
    "        # Apply the logic and insert the \"Post Type\" column\n",
    "        df.insert(content_index, \"Post Type\", df.apply(determine_post_type, axis=1))\n",
    "    else:\n",
    "        print(f\"Warning: 'Content' or 'Source_Type' column not found in sheet '{sheet_name}'\")\n",
    "\n",
    "    # Add the processed sheet back to the dictionary\n",
    "    processed_sheets[sheet_name] = df\n",
    "\n",
    "# Save the processed sheets back to a new Excel file\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for sheet_name, df in processed_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"File saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "903136ab-f7b3-4abf-81d5-f700f132a118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     WATCH NOW! New Documentary! LIFE AND DEATH Les...\n",
      "1     Rob Oliphant,MP is a sincere guy.But as I have...\n",
      "2     The words below are from Canada’s UN ambassado...\n",
      "3     It’s been a month since I was visiting survivo...\n",
      "4     Sometimes the simplest explanation is the best...\n",
      "5     RT @Stockwell_Day: Whatever side you’re on,and...\n",
      "6     A reflection for those who support Hamas over ...\n",
      "7     Inside Israel: Forum Daily Special Report http...\n",
      "8     Look at the photo. Before evil global militant...\n",
      "9     Just back from Israel with footage+interviews ...\n",
      "10    It’s been a month since I returned from being ...\n",
      "11    This is the 2nd episode (3rd will be out in a ...\n",
      "12    ⁦GRAMMY GLITCH?@CBCNews⁩ went silent +⁦@CNN⁩+o...\n",
      "13    Elon, good job reposting Sen.Fetterman, strong...\n",
      "14    The Israeli Left’s New Military Messiah. If yo...\n",
      "15    For years now some of us said the Hamas terror...\n",
      "16    Supporting a cause is fine.I support the cause...\n",
      "17    As in almost every gun crime iincident,if the ...\n",
      "18    The words below are from Canada’s ambassador t...\n",
      "19    See items below, direct from my visit to kibbu...\n",
      "20    I’m still in Israel in the conflict zones,meet...\n",
      "21    On Int’l Nat’l Holocaust Day we have an astoni...\n",
      "22    Sadly,@CBCNews @cnnbrk @BBCBreaking etc contin...\n",
      "23    You may agree or not,but here is an Israeli me...\n",
      "24    At 16 my friends+I were in a rigorous educatio...\n",
      "25    Thanks for the feedback but we must continue t...\n",
      "26    Dear PM @JustinTrudeau. Sir,you let a mob of J...\n",
      "27    Whatever side you’re on,and remembering Hamas ...\n",
      "28    With respect,the Muslim Council + @theJagmeetS...\n",
      "29    Give credit to the Liberals. Many of us have a...\n",
      "30    One of the funniest things (and he is funny)ab...\n",
      "Name: Content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Content\"].head(50))  # Print first 10 rows of Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3aea820-45e4-4ec4-9a51-14f8c346d875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Twitter']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Source_Type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3198aa-f9a6-4086-9084-62ac398cbe51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
